import sys
import streamlit as st
import tempfile
import array

import os
from dotenv import load_dotenv


import oracledb
from langchain_community.vectorstores import oraclevs
from langchain_community.vectorstores.oraclevs import OracleVS
from langchain_community.document_loaders import PyPDFLoader

from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import BaseDocumentTransformer, Document

from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import UpstageEmbeddings, UpstageLayoutAnalysisLoader
from langchain_upstage import ChatUpstage
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

import warnings
warnings.filterwarnings("ignore")

# add api code and db user info into secrets.toml to run in local
# % streamlit run ui.py to run

username=os.environ["DB_USER"]
password=os.environ["DB_PASSWORD"]
dsn=os.environ["DSN"]

con = oracledb.connect(user=username, password=password, dsn=dsn)

try: 
    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)
    print("Connection successful!", conn23c.version)
except Exception as e:
    print("Connection failed!")

st.title("Summacum")
st.write("---")

uploaded_file = st.file_uploader("Choose a file to study",type=['pdf'])
st.write("---")

def preprocess(uploaded_file):
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
    with open(temp_filepath, "wb") as f:
        f.write(uploaded_file.getbuffer())

    file_path = temp_filepath

    layzer = UpstageLayoutAnalysisLoader(file_path,split="page")
    text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1500, chunk_overlap=200, language=Language.HTML
)
    docs = layzer.load()
    docs = text_splitter.split_documents(docs)
    '''for doc in docs:
        doc.metadata['title']="Biomedical Engineering_ Bridging Medicine and Technology"
    '''
    
    return docs

def embedding(docs):
    upstage_embeddings = UpstageEmbeddings(model="solar-embedding-1-large")
     
# Configure the vector store with the model, table name, and using the indicated distance strategy for the similarity search and vectorize the chunks

    knowledge_base = OracleVS.from_documents(docs, upstage_embeddings, client=conn23c, 
                    table_name="biomedical_table", 
                    distance_strategy=DistanceStrategy.DOT_PRODUCT)
    
    upstage_embeddings = UpstageEmbeddings(model="solar-embedding-1-large")

    vector_store = OracleVS(client=conn23c, 
                        embedding_function=upstage_embeddings, 
                        table_name="biomedical_table", 
                        distance_strategy=DistanceStrategy.DOT_PRODUCT)

    retriever = vector_store.as_retriever()
    
    return retriever, vector_store

def get_data():
    uploaded_problem = None
    uploaded_answer = None
 
    uploaded_problem = st.file_uploader("give a problem",type=['pdf'])
    uploaded_answer = st.file_uploader("give an answer",type=['pdf'])  
    st.write("---") 
    if (uploaded_answer is not None) and (uploaded_problem is not None):
        prob_docs = preprocess(uploaded_problem)
        prob_sols = preprocess(uploaded_answer)
    
        return prob_docs, prob_sols

def main():
    if uploaded_file is not None:
        docs = preprocess(uploaded_file) 
        retriever, vector_store = embedding(docs)
        #prb_docs, sol_docs = get_data()
        from langchain_upstage import UpstageGroundednessCheck

        groundedness_check = UpstageGroundednessCheck()
        llm = ChatUpstage()
        template = """Please give me the solutions from quiz referencing the below knowledge:
              ----------
              {context}
              ----------
              Quiz: {quiz}
              """
        prompt = PromptTemplate.from_template(template)
        k=2
        grounded = False
        while not grounded:
            retriever = vector_store.as_retriever(k=k)

            chain = (
    # {"context": retriever, "quiz": RunnablePassthrough(), "sol": RunnablePassthrough()}
                {"context": retriever, "quiz": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
                    )
  # response = chain.invoke({"quiz":prb_docs[0], "sol":sol_docs[0]})
            prb = ""
            with open("./prob.txt", "r") as f:
                prb =f.read()
            response = chain.invoke({"quiz":prb})
            st.write("Bot response: ", response)

  
            result_chunks=vector_store.similarity_search(query = prb, k=k)
            ref = ""
            for i in range(k):
                ref += result_chunks[i].page_content
                ref += "\n"
                gc_result = groundedness_check.invoke({"context": ref, "answer": response})

            if gc_result.lower().startswith("grounded"):
                grounded = True
                ref_parser = ChatUpstage()
                template = """Please give me the content from HTML formatted given content:
              ----------
              {context}
              ----------
              """
                prompt = PromptTemplate.from_template(template)


                chain = (
                    {"context": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
                    )
                # response = chain.invoke({"quiz":prb_docs[0], "sol":sol_docs[0]})
                response = chain.invoke({"context":ref})

                

                st.write(f"Answer is grounded\n[response] {response}")
            else:
                st.write(f"answer is not grounded with k={k}")
        

        
           
    
if __name__ == "__main__":
    main()
    